{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = arff.loadarff('1year.arff')\n",
    "all_data = pd.DataFrame(raw_data[0])\n",
    "\n",
    "x='X'\n",
    "col = [str(x)+str(i) for i in range(1,66)]\n",
    "all_data.columns=col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7027, 65)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>X24</th>\n",
       "      <th>X25</th>\n",
       "      <th>X26</th>\n",
       "      <th>X27</th>\n",
       "      <th>X28</th>\n",
       "      <th>X29</th>\n",
       "      <th>X30</th>\n",
       "      <th>X31</th>\n",
       "      <th>X32</th>\n",
       "      <th>X33</th>\n",
       "      <th>X34</th>\n",
       "      <th>X35</th>\n",
       "      <th>X36</th>\n",
       "      <th>X37</th>\n",
       "      <th>X38</th>\n",
       "      <th>X39</th>\n",
       "      <th>X40</th>\n",
       "      <th>X41</th>\n",
       "      <th>X42</th>\n",
       "      <th>X43</th>\n",
       "      <th>X44</th>\n",
       "      <th>X45</th>\n",
       "      <th>X46</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>X58</th>\n",
       "      <th>X59</th>\n",
       "      <th>X60</th>\n",
       "      <th>X61</th>\n",
       "      <th>X62</th>\n",
       "      <th>X63</th>\n",
       "      <th>X64</th>\n",
       "      <th>X65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.39641</td>\n",
       "      <td>2.0472</td>\n",
       "      <td>32.3510</td>\n",
       "      <td>0.38825</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.33050</td>\n",
       "      <td>1.1389</td>\n",
       "      <td>0.50494</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>0.65980</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>497.42</td>\n",
       "      <td>0.73378</td>\n",
       "      <td>2.6349</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>0.149420</td>\n",
       "      <td>43.370</td>\n",
       "      <td>1.2479</td>\n",
       "      <td>0.21402</td>\n",
       "      <td>0.119980</td>\n",
       "      <td>0.47706</td>\n",
       "      <td>0.50494</td>\n",
       "      <td>0.60411</td>\n",
       "      <td>1.45820</td>\n",
       "      <td>1.7615</td>\n",
       "      <td>5.9443</td>\n",
       "      <td>0.11788</td>\n",
       "      <td>0.149420</td>\n",
       "      <td>94.14</td>\n",
       "      <td>3.8772</td>\n",
       "      <td>0.56393</td>\n",
       "      <td>0.21402</td>\n",
       "      <td>1.7410</td>\n",
       "      <td>593.2700</td>\n",
       "      <td>0.50591</td>\n",
       "      <td>0.128040</td>\n",
       "      <td>0.662950</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.128040</td>\n",
       "      <td>114.42</td>\n",
       "      <td>71.050</td>\n",
       "      <td>1.00970</td>\n",
       "      <td>1.52250</td>\n",
       "      <td>49.394</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.110850</td>\n",
       "      <td>2.0420</td>\n",
       "      <td>0.37854</td>\n",
       "      <td>0.25792</td>\n",
       "      <td>2.2437</td>\n",
       "      <td>2.2480</td>\n",
       "      <td>348690.0</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.39718</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.4277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.47225</td>\n",
       "      <td>1.9447</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.99601</td>\n",
       "      <td>1.6996</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.261140</td>\n",
       "      <td>0.51680</td>\n",
       "      <td>0.158350</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>677.96</td>\n",
       "      <td>0.53838</td>\n",
       "      <td>2.0005</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>87.981</td>\n",
       "      <td>1.4293</td>\n",
       "      <td>0.24806</td>\n",
       "      <td>0.123040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.39542</td>\n",
       "      <td>0.43992</td>\n",
       "      <td>88.44400</td>\n",
       "      <td>16.9460</td>\n",
       "      <td>3.6884</td>\n",
       "      <td>0.26969</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>122.17</td>\n",
       "      <td>2.9876</td>\n",
       "      <td>2.98760</td>\n",
       "      <td>0.20616</td>\n",
       "      <td>1.6996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.086422</td>\n",
       "      <td>0.064371</td>\n",
       "      <td>0.145950</td>\n",
       "      <td>199.49</td>\n",
       "      <td>111.510</td>\n",
       "      <td>0.51045</td>\n",
       "      <td>1.12520</td>\n",
       "      <td>100.130</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>0.139610</td>\n",
       "      <td>1.9447</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.33472</td>\n",
       "      <td>17.8660</td>\n",
       "      <td>17.8660</td>\n",
       "      <td>2304.6</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.42002</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.350</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>60.9870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.26713</td>\n",
       "      <td>1.5548</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.43695</td>\n",
       "      <td>1.3090</td>\n",
       "      <td>0.30408</td>\n",
       "      <td>0.312580</td>\n",
       "      <td>0.64184</td>\n",
       "      <td>0.244350</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>794.16</td>\n",
       "      <td>0.45961</td>\n",
       "      <td>1.4369</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>73.133</td>\n",
       "      <td>1.4283</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.189960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.28932</td>\n",
       "      <td>0.37282</td>\n",
       "      <td>86.01100</td>\n",
       "      <td>1.0627</td>\n",
       "      <td>4.3749</td>\n",
       "      <td>0.41929</td>\n",
       "      <td>0.238150</td>\n",
       "      <td>176.93</td>\n",
       "      <td>2.0630</td>\n",
       "      <td>1.42740</td>\n",
       "      <td>0.31565</td>\n",
       "      <td>1.3090</td>\n",
       "      <td>2.3019</td>\n",
       "      <td>0.51537</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.322020</td>\n",
       "      <td>0.074020</td>\n",
       "      <td>0.231170</td>\n",
       "      <td>165.51</td>\n",
       "      <td>92.381</td>\n",
       "      <td>0.94807</td>\n",
       "      <td>1.01010</td>\n",
       "      <td>96.372</td>\n",
       "      <td>0.291810</td>\n",
       "      <td>0.222930</td>\n",
       "      <td>1.0758</td>\n",
       "      <td>0.48152</td>\n",
       "      <td>0.48474</td>\n",
       "      <td>1.2098</td>\n",
       "      <td>2.0504</td>\n",
       "      <td>6332.7</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.81774</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.270</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.2078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>2.4928</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.14988</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.86610</td>\n",
       "      <td>1.0571</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>0.30163</td>\n",
       "      <td>0.094257</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>917.01</td>\n",
       "      <td>0.39803</td>\n",
       "      <td>3.2537</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>0.071428</td>\n",
       "      <td>79.788</td>\n",
       "      <td>1.5069</td>\n",
       "      <td>0.11550</td>\n",
       "      <td>0.062782</td>\n",
       "      <td>0.17193</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>0.36152</td>\n",
       "      <td>0.94076</td>\n",
       "      <td>1.9618</td>\n",
       "      <td>4.6511</td>\n",
       "      <td>0.14343</td>\n",
       "      <td>0.071428</td>\n",
       "      <td>91.37</td>\n",
       "      <td>3.9948</td>\n",
       "      <td>0.37581</td>\n",
       "      <td>0.11550</td>\n",
       "      <td>1.3562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>0.088995</td>\n",
       "      <td>0.401390</td>\n",
       "      <td>0.069622</td>\n",
       "      <td>0.088995</td>\n",
       "      <td>180.77</td>\n",
       "      <td>100.980</td>\n",
       "      <td>0.28720</td>\n",
       "      <td>1.56960</td>\n",
       "      <td>84.344</td>\n",
       "      <td>0.085874</td>\n",
       "      <td>0.066165</td>\n",
       "      <td>2.4928</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.25033</td>\n",
       "      <td>2.4524</td>\n",
       "      <td>2.4524</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.14207</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.5497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.61323</td>\n",
       "      <td>0.22960</td>\n",
       "      <td>1.4063</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.63070</td>\n",
       "      <td>1.1559</td>\n",
       "      <td>0.38677</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.33147</td>\n",
       "      <td>0.121820</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>1133.20</td>\n",
       "      <td>0.32211</td>\n",
       "      <td>1.6307</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.115530</td>\n",
       "      <td>57.045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19832</td>\n",
       "      <td>0.115530</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.38677</td>\n",
       "      <td>0.32211</td>\n",
       "      <td>1.41380</td>\n",
       "      <td>1.1184</td>\n",
       "      <td>4.1424</td>\n",
       "      <td>0.27884</td>\n",
       "      <td>0.115530</td>\n",
       "      <td>147.04</td>\n",
       "      <td>2.4823</td>\n",
       "      <td>0.32340</td>\n",
       "      <td>0.19832</td>\n",
       "      <td>1.6278</td>\n",
       "      <td>11.2470</td>\n",
       "      <td>0.43489</td>\n",
       "      <td>0.122310</td>\n",
       "      <td>0.293040</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.122310</td>\n",
       "      <td>141.62</td>\n",
       "      <td>84.574</td>\n",
       "      <td>0.73919</td>\n",
       "      <td>0.95787</td>\n",
       "      <td>65.936</td>\n",
       "      <td>0.188110</td>\n",
       "      <td>0.116010</td>\n",
       "      <td>1.2959</td>\n",
       "      <td>0.56511</td>\n",
       "      <td>0.40285</td>\n",
       "      <td>1.8839</td>\n",
       "      <td>2.1184</td>\n",
       "      <td>3186.6</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.48431</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.210</td>\n",
       "      <td>2.8692</td>\n",
       "      <td>7.8980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1       X2       X3      X4       X5       X6        X7       X8  \\\n",
       "0  0.200550  0.37951  0.39641  2.0472  32.3510  0.38825  0.249760  1.33050   \n",
       "1  0.209120  0.49988  0.47225  1.9447  14.7860  0.00000  0.258340  0.99601   \n",
       "2  0.248660  0.69592  0.26713  1.5548  -1.1523  0.00000  0.309060  0.43695   \n",
       "3  0.081483  0.30734  0.45879  2.4928  51.9520  0.14988  0.092704  1.86610   \n",
       "4  0.187320  0.61323  0.22960  1.4063  -7.3128  0.18732  0.187320  0.63070   \n",
       "\n",
       "       X9      X10       X11      X12       X13       X14      X15      X16  \\\n",
       "0  1.1389  0.50494  0.249760  0.65980  0.166600  0.249760   497.42  0.73378   \n",
       "1  1.6996  0.49788  0.261140  0.51680  0.158350  0.258340   677.96  0.53838   \n",
       "2  1.3090  0.30408  0.312580  0.64184  0.244350  0.309060   794.16  0.45961   \n",
       "3  1.0571  0.57353  0.092704  0.30163  0.094257  0.092704   917.01  0.39803   \n",
       "4  1.1559  0.38677  0.187320  0.33147  0.121820  0.187320  1133.20  0.32211   \n",
       "\n",
       "      X17       X18       X19     X20     X21      X22       X23      X24  \\\n",
       "0  2.6349  0.249760  0.149420  43.370  1.2479  0.21402  0.119980  0.47706   \n",
       "1  2.0005  0.258340  0.152000  87.981  1.4293  0.24806  0.123040      NaN   \n",
       "2  1.4369  0.309060  0.236100  73.133  1.4283  0.30260  0.189960      NaN   \n",
       "3  3.2537  0.092704  0.071428  79.788  1.5069  0.11550  0.062782  0.17193   \n",
       "4  1.6307  0.187320  0.115530  57.045     NaN  0.19832  0.115530  0.18732   \n",
       "\n",
       "       X25      X26       X27      X28     X29      X30       X31     X32  \\\n",
       "0  0.50494  0.60411   1.45820   1.7615  5.9443  0.11788  0.149420   94.14   \n",
       "1  0.39542  0.43992  88.44400  16.9460  3.6884  0.26969  0.152000  122.17   \n",
       "2  0.28932  0.37282  86.01100   1.0627  4.3749  0.41929  0.238150  176.93   \n",
       "3  0.57353  0.36152   0.94076   1.9618  4.6511  0.14343  0.071428   91.37   \n",
       "4  0.38677  0.32211   1.41380   1.1184  4.1424  0.27884  0.115530  147.04   \n",
       "\n",
       "      X33      X34      X35     X36       X37      X38       X39       X40  \\\n",
       "0  3.8772  0.56393  0.21402  1.7410  593.2700  0.50591  0.128040  0.662950   \n",
       "1  2.9876  2.98760  0.20616  1.6996       NaN  0.49788  0.121300  0.086422   \n",
       "2  2.0630  1.42740  0.31565  1.3090    2.3019  0.51537  0.241140  0.322020   \n",
       "3  3.9948  0.37581  0.11550  1.3562       NaN  0.57353  0.088995  0.401390   \n",
       "4  2.4823  0.32340  0.19832  1.6278   11.2470  0.43489  0.122310  0.293040   \n",
       "\n",
       "        X41       X42     X43      X44      X45      X46      X47       X48  \\\n",
       "0  0.051402  0.128040  114.42   71.050  1.00970  1.52250   49.394  0.185300   \n",
       "1  0.064371  0.145950  199.49  111.510  0.51045  1.12520  100.130  0.237270   \n",
       "2  0.074020  0.231170  165.51   92.381  0.94807  1.01010   96.372  0.291810   \n",
       "3  0.069622  0.088995  180.77  100.980  0.28720  1.56960   84.344  0.085874   \n",
       "4  0.096680  0.122310  141.62   84.574  0.73919  0.95787   65.936  0.188110   \n",
       "\n",
       "        X49     X50      X51      X52      X53      X54       X55       X56  \\\n",
       "0  0.110850  2.0420  0.37854  0.25792   2.2437   2.2480  348690.0  0.121960   \n",
       "1  0.139610  1.9447  0.49988  0.33472  17.8660  17.8660    2304.6  0.121300   \n",
       "2  0.222930  1.0758  0.48152  0.48474   1.2098   2.0504    6332.7  0.241140   \n",
       "3  0.066165  2.4928  0.30734  0.25033   2.4524   2.4524   20545.0  0.054015   \n",
       "4  0.116010  1.2959  0.56511  0.40285   1.8839   2.1184    3186.6  0.134850   \n",
       "\n",
       "       X57      X58       X59     X60     X61      X62     X63      X64 X65  \n",
       "0  0.39718  0.87804  0.001924  8.4160  5.1372   82.658  4.4158   7.4277   0  \n",
       "1  0.42002  0.85300  0.000000  4.1486  3.2732  107.350  3.4000  60.9870   0  \n",
       "2  0.81774  0.76599  0.694840  4.9909  3.9510  134.270  2.7185   5.2078   0  \n",
       "3  0.14207  0.94598  0.000000  4.5746  3.6147   86.435  4.2228   5.5497   0  \n",
       "4  0.48431  0.86515  0.124440  6.3985  4.3158  127.210  2.8692   7.8980   0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 70)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritesh.kankonkar\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in sqrt\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"imp_median = Imputer(missing_values = 'NaN', strategy='median')\\nfor i in range(1,65):\\n   imp_median.fit(X_train[['X'+str(i)]])\\n   X_train['X'+str(i)] = imp_median.transform(X_train[['X'+str(i)]])\\n    \\n    \\nimp_median = Imputer(missing_values = 'NaN', strategy='median')\\nfor i in range(1,65):\\n   imp_median.fit(X_test[['X'+str(i)]])\\n   X_test['X'+str(i)] = imp_median.transform(X_test[['X'+str(i)]])\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = all_data.drop(['X65'], axis=1)\n",
    "y = all_data['X65']\n",
    "\n",
    "imp_median = Imputer(missing_values = 'NaN', strategy='median')\n",
    "\n",
    "X = np.sqrt(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "for i in range(1,65):\n",
    "   imp_median.fit(X[['X'+str(i)]])\n",
    "   X['X'+str(i)] = imp_median.transform(X[['X'+str(i)]])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=9)\n",
    "\n",
    "'''imp_median = Imputer(missing_values = 'NaN', strategy='median')\n",
    "for i in range(1,65):\n",
    "   imp_median.fit(X_train[['X'+str(i)]])\n",
    "   X_train['X'+str(i)] = imp_median.transform(X_train[['X'+str(i)]])\n",
    "    \n",
    "    \n",
    "imp_median = Imputer(missing_values = 'NaN', strategy='median')\n",
    "for i in range(1,65):\n",
    "   imp_median.fit(X_test[['X'+str(i)]])\n",
    "   X_test['X'+str(i)] = imp_median.transform(X_test[['X'+str(i)]])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=9, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_1 = RandomForestClassifier(max_depth=20, random_state=9, max_features=\"log2\")\n",
    "clf_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96443812233285919"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = clf_1.predict(X_test)\n",
    "accuracy_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01522003,  0.00997973,  0.02038292,  0.02190191,  0.0184432 ,\n",
       "        0.01372634,  0.02373216,  0.01234478,  0.01581896,  0.01067723,\n",
       "        0.02745357,  0.01399832,  0.01721512,  0.01252853,  0.01703278,\n",
       "        0.01921824,  0.00789905,  0.00777155,  0.01926199,  0.01070088,\n",
       "        0.01362294,  0.01563864,  0.01585207,  0.02800107,  0.01669029,\n",
       "        0.01328818,  0.04346182,  0.01691241,  0.03045499,  0.01175874,\n",
       "        0.00913902,  0.00810438,  0.01126801,  0.02060911,  0.01382771,\n",
       "        0.01495124,  0.0115666 ,  0.00955824,  0.01402372,  0.02121962,\n",
       "        0.01559415,  0.01305761,  0.01045047,  0.01491519,  0.0163605 ,\n",
       "        0.03720537,  0.01393326,  0.01464078,  0.01176326,  0.01118997,\n",
       "        0.01357412,  0.0118153 ,  0.01269671,  0.01434969,  0.01233235,\n",
       "        0.01286979,  0.01068417,  0.02868961,  0.0086851 ,  0.01401388,\n",
       "        0.01477447,  0.01143639,  0.00780867,  0.01190308])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98       703\n",
      "          1       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.96      0.98       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X1', 0.015220029293086929], ['X2', 0.0099797341989026657], ['X3', 0.020382920847130958], ['X4', 0.021901914136807171], ['X5', 0.018443200418682554], ['X6', 0.013726343659559322], ['X7', 0.023732159820099692], ['X8', 0.012344778093567391], ['X9', 0.015818956212887952], ['X10', 0.010677228980411837], ['X11', 0.027453570192085802], ['X12', 0.013998316706305095], ['X13', 0.017215119767269117], ['X14', 0.012528526076921034], ['X15', 0.017032776876643653], ['X16', 0.019218243929274535], ['X17', 0.0078990528119273241], ['X18', 0.0077715454327492457], ['X19', 0.019261989904569968], ['X20', 0.010700881102555709], ['X21', 0.013622939155351102], ['X22', 0.015638637451954297], ['X23', 0.015852071179487039], ['X24', 0.028001074968654477], ['X25', 0.016690285933832722], ['X26', 0.013288184553282078], ['X27', 0.043461819063863934], ['X28', 0.016912408905260849], ['X29', 0.030454993315892777], ['X30', 0.011758743000282975], ['X31', 0.0091390183629323531], ['X32', 0.0081043781621579665], ['X33', 0.011268012480650555], ['X34', 0.020609106074927032], ['X35', 0.013827711718810348], ['X36', 0.014951242581275661], ['X37', 0.011566596851163184], ['X38', 0.0095582365654165967], ['X39', 0.014023724911256969], ['X40', 0.021219622508013635], ['X41', 0.015594145081106981], ['X42', 0.013057612981493291], ['X43', 0.01045047457108975], ['X44', 0.014915194699624029], ['X45', 0.016360497135522994], ['X46', 0.037205368270950043], ['X47', 0.013933261669289243], ['X48', 0.014640782106045811], ['X49', 0.01176326063106351], ['X50', 0.011189968766496699], ['X51', 0.013574117722499893], ['X52', 0.01181530135583903], ['X53', 0.01269671452280908], ['X54', 0.014349686753480622], ['X55', 0.012332352329230556], ['X56', 0.012869786545150867], ['X57', 0.010684173023949615], ['X58', 0.028689607657177717], ['X59', 0.0086851043533237287], ['X60', 0.014013881044469967], ['X61', 0.014774467672797862], ['X62', 0.011436394295197105], ['X63', 0.0078086660203984682], ['X64', 0.01190308458508863]]\n"
     ]
    }
   ],
   "source": [
    "labels = X_train.columns\n",
    "lst=[]\n",
    "for feature in zip(labels, clf_1.feature_importances_):\n",
    "    lst.append(list(feature))\n",
    "\n",
    "print lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst.sort(reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'X1', u'X2', u'X3', u'X4', u'X5', u'X6', u'X7', u'X8', u'X9', u'X10',\n",
       "       u'X11', u'X12', u'X13', u'X14', u'X15', u'X16', u'X17', u'X18', u'X19',\n",
       "       u'X20', u'X21', u'X22', u'X23', u'X24', u'X25', u'X26', u'X27', u'X28',\n",
       "       u'X29', u'X30', u'X31', u'X32', u'X33', u'X34', u'X35', u'X36', u'X37',\n",
       "       u'X38', u'X39', u'X40', u'X41', u'X42', u'X43', u'X44', u'X45', u'X46',\n",
       "       u'X47', u'X48', u'X49', u'X50', u'X51', u'X52', u'X53', u'X54', u'X55',\n",
       "       u'X56', u'X57', u'X58', u'X59', u'X60', u'X61', u'X62', u'X63', u'X64'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummyX = X_train[['X24','X27','X34','X46','X58','X44','X16','X35']]\n",
    "dummyXtest = X_test[['X24','X27','X34','X46','X58','X44','X16','X35']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.964438122333\n",
      "AUC Curve:  0.5\n",
      "Recall score: 0.0\n",
      "Accuracy:  0.964438122333\n",
      "AUC Curve:  0.5\n",
      "Recall score: 0.0\n",
      "Accuracy:  0.963015647226\n",
      "AUC Curve:  0.499262536873\n",
      "Recall score: 0.0\n",
      "Accuracy:  0.964438122333\n",
      "AUC Curve:  0.5\n",
      "Recall score: 0.0\n",
      "Accuracy:  0.958748221906\n",
      "AUC Curve:  0.497050147493\n",
      "Recall score: 0.0\n",
      "Accuracy:  0.961593172119\n",
      "AUC Curve:  0.498525073746\n",
      "Recall score: 0.0\n",
      "Accuracy:  0.961593172119\n",
      "AUC Curve:  0.498525073746\n",
      "Recall score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#clf_2 = RandomForestClassifier(max_depth=10, random_state=9)\n",
    "#clf_2.fit(dummyX, y_train)\n",
    "\n",
    "#logreg = LogisticRegression()\n",
    "#logreg.fit(X_train, y_train)\n",
    "\n",
    "for i in range(2,9):\n",
    "   clf_entropy = DecisionTreeClassifier(criterion='entropy',max_depth=i)\n",
    "   clf_entropy.fit(dummyX,y_train)\n",
    "   y_prediction_entropy = clf_entropy.predict(dummyXtest)\n",
    "   accuracy_entropy = accuracy_score(y_test,y_prediction_entropy)\n",
    "   #print \"Max_depth: \", i\n",
    "   print \"Accuracy: \", accuracy_entropy\n",
    "   print \"AUC Curve: \", roc_auc_score(y_test,y_prediction_entropy)\n",
    "   print \"Recall score:\", recall_score(y_test, y_prediction_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.482168330956\n"
     ]
    }
   ],
   "source": [
    "print roc_auc_score(y_prediction_entropy,y_test)\n",
    "\n",
    "#print type(y_test)\n",
    "#print type(y_prediction_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.98       699\n",
      "          1       0.00      0.00      0.00         4\n",
      "\n",
      "avg / total       0.99      0.96      0.97       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prediction = clf_2.predict(dummyXtest)\n",
    "#print classification_report(prediction, y_test)\n",
    "\n",
    "log_pred = logreg.predict(X_test)\n",
    "#print classification_report(log_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "for i in [0.01,0.02,0.2,0.3,0.4,0.5]:\n",
    "   lassoreg = Lasso(alpha=i, normalize=True)\n",
    "   lassoreg.fit(X_train, y_train)\n",
    "   y_pred2 = lassoreg.predict(X_test)\n",
    "   #z = lassoreg.predict_proba()\n",
    "    \n",
    "#classification_report(y_pred2, y_test)\n",
    "#print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "auc(y_pred2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf1 = BaggingClassifier(LogisticRegression(), n_estimators=100, max_samples=100, \n",
    "                                bootstrap=True, random_state=9)\n",
    "\n",
    "bagging_clf1.fit(X_train, y_train)\n",
    "y_pred_bagging1 = bagging_clf1.predict(X_test)\n",
    "score_bc_lr = accuracy_score(y_test, y_pred_bagging1)\n",
    "\n",
    "bagging_clf2 = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100,\n",
    "                                bootstrap=True, random_state=9)\n",
    "\n",
    "bagging_clf2.fit(X_train, y_train)\n",
    "y_pred_bagging2 = bagging_clf2.predict(X_test)\n",
    "score_bc_dt = accuracy_score(y_test, y_pred_bagging2)\n",
    "\n",
    "bagging_clf3 = BaggingClassifier(RandomForestClassifier(), n_estimators=100, max_samples=100,\n",
    "                                bootstrap=True, random_state=9)\n",
    "\n",
    "bagging_clf3.fit(X_train, y_train)\n",
    "y_pred_bagging3 = bagging_clf2.predict(X_test)\n",
    "score_bc_rf = accuracy_score(y_test, y_pred_bagging3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "voting_clf_soft = VotingClassifier(estimators = [('bagging_lr', bagging_clf1), \n",
    "                                                 ('bagging_dt', bagging_clf2),\n",
    "                                                 ('bagging_rf', bagging_clf3)],voting='soft')\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "voting_prediction = voting_clf_soft.predict(X_test)\n",
    "score_voting = accuracy_score(voting_prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964438122333\n",
      "0.964438122333\n",
      "0.964438122333\n",
      "0.964438122333\n"
     ]
    }
   ],
   "source": [
    "print score_bc_dt\n",
    "print score_bc_lr\n",
    "print score_bc_rf\n",
    "print score_voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-532926a90583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoting_prediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ritesh.kankonkar\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m    275\u001b[0m     return _average_binary_score(\n\u001b[0;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritesh.kankonkar\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\metrics\\base.pyc\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritesh.kankonkar\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[0;32m    269\u001b[0m                              \"is not defined in that case.\")\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "print roc_auc_score(voting_prediction,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
